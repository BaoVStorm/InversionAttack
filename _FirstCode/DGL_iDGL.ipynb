{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import pickle\n",
    "import math\n",
    "import PIL.Image as Image\n",
    "\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, channel=3, hideen=768, num_classes=10):\n",
    "        super(LeNet, self).__init__()\n",
    "        act = nn.Sigmoid\n",
    "        self.body = nn.Sequential(\n",
    "            nn.Conv2d(channel, 12, kernel_size=5, padding=5 // 2, stride=2),\n",
    "            act(),\n",
    "            nn.Conv2d(12, 12, kernel_size=5, padding=5 // 2, stride=2),\n",
    "            act(),\n",
    "            nn.Conv2d(12, 12, kernel_size=5, padding=5 // 2, stride=1),\n",
    "            act(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hideen, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.body(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "def weights_init(m):\n",
    "    try:\n",
    "        if hasattr(m, \"weight\"):\n",
    "            m.weight.data.uniform_(-0.5, 0.5)\n",
    "    except Exception:\n",
    "        print('warning: failed in weights_init for %s.weight' % m._get_name())\n",
    "    try:\n",
    "        if hasattr(m, \"bias\"):\n",
    "            m.bias.data.uniform_(-0.5, 0.5)\n",
    "    except Exception:\n",
    "        print('warning: failed in weights_init for %s.bias' % m._get_name())\n",
    "\n",
    "class Dataset_from_Image(Dataset):\n",
    "    def __init__(self, imgs, labs, transform=None):\n",
    "        self.imgs = imgs # img paths\n",
    "        self.labs = labs # labs is ndarray\n",
    "        self.transform = transform\n",
    "        del imgs, labs\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labs.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lab = self.labs[idx]\n",
    "        img = Image.open(self.imgs[idx])\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        return img, lab\n",
    "\n",
    "def lfw_dataset(lfw_path, shape_img):\n",
    "    images_all = []\n",
    "    labels_all = []\n",
    "    folders = os.listdir(lfw_path)\n",
    "    for foldidx, fold in enumerate(folders):\n",
    "        files = os.listdir(os.path.join(lfw_path, fold))\n",
    "        for f in files:\n",
    "            if len(f) > 4 and f[-4:] == '.jpg':\n",
    "                images_all.append(os.path.join(lfw_path, fold, f))\n",
    "                labels_all.append(foldidx)\n",
    "\n",
    "    transform = transforms.Compose([transforms.Resize(size=shape_img)])\n",
    "    dst = Dataset_from_Image(images_all, np.asarray(labels_all, dtype=int), transform=transform)\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Visualize_Loss(type, method, epoch_points, history, image_history, save_path, imidx_value, imidx_list):\n",
    "    # Create visualization with images and loss values\n",
    "    fig, ax = plt.subplots(figsize=(25, 12))\n",
    "\n",
    "    history = history[::10]\n",
    "\n",
    "    # print(\"epoch_points: \", epoch_points )\n",
    "    # print(\"history: \", history )\n",
    "    # print(\"image_history: \", image_history )\n",
    "\n",
    "    # Plot loss curve \n",
    "    ax.plot(epoch_points, history, 'b-', linewidth=2, zorder=1)\n",
    "    ax.set_xlabel('Epochs', fontsize=12)\n",
    "    ax.set_ylabel(f'L2 {type}', fontsize=12)\n",
    "    ax.set_title(f'Epoch - {type} progress', fontsize=14)\n",
    "\n",
    "    # Set logarithmic scale\n",
    "    ax.set_yscale('log')\n",
    "    min_value = 1e-6\n",
    "    while min_value * 10 <= min(history):\n",
    "        min_value *= 10\n",
    "\n",
    "    ax.set_ylim(min_value, max(history))\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add images and loss values\n",
    "    for x, y, img in zip(epoch_points, history, image_history):\n",
    "        # Add image\n",
    "        imagebox = OffsetImage(np.array(img),\n",
    "                            zoom=0.8,\n",
    "                            resample=True)\n",
    "        \n",
    "        ab_img = AnnotationBbox(imagebox, (x, y),\n",
    "                            xybox=(0, 40),\n",
    "                            xycoords='data',\n",
    "                            boxcoords=\"offset points\",\n",
    "                            frameon=True,\n",
    "                            bboxprops=dict(facecolor='white',\n",
    "                                            edgecolor='gray',\n",
    "                                            alpha=0.9))\n",
    "\n",
    "        ax.add_artist(ab_img)\n",
    "\n",
    "        # Add loss value text below image\n",
    "        ax.annotate(f'{y:.2e}',  # Scientific notation\n",
    "                    xy=(x, y),\n",
    "                    xytext=(0, -20),  # Position below point\n",
    "                    textcoords='offset points',\n",
    "                    ha='center',\n",
    "                    va='top',\n",
    "                    bbox=dict(facecolor='white',\n",
    "                            edgecolor='none',\n",
    "                            alpha=0.7))\n",
    "\n",
    "    plt.subplots_adjust(bottom=0.2)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('%s/%05d_%s_%s_on_%s.png' % (save_path, imidx_value, method, type, imidx_list))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(dataset = 'MNIST'):\n",
    "    print(\"=============== Load in dataset ===============\")\n",
    "\n",
    "    root_path = '.'\n",
    "    data_path = os.path.join(root_path, '../data').replace('\\\\', '/')\n",
    "    save_path = os.path.join(root_path, 'results/iDLG_%s'%dataset).replace('\\\\', '/')\n",
    "    \n",
    "    lr = 1.0\n",
    "    num_dummy = 1\n",
    "    Iteration = 300\n",
    "    num_exp = 80\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = 'cuda' if use_cuda else 'cpu'\n",
    "\n",
    "    tt = transforms.Compose([transforms.ToTensor()])\n",
    "    tp = transforms.Compose([transforms.ToPILImage()])\n",
    "\n",
    "    print(dataset, 'root_path:', root_path)\n",
    "    print(dataset, 'data_path:', data_path)\n",
    "    print(dataset, 'save_path:', save_path)\n",
    "\n",
    "    if not os.path.exists('results'):\n",
    "        os.mkdir('results')\n",
    "    if not os.path.exists(save_path):\n",
    "        os.mkdir(save_path)\n",
    "\n",
    "    ''' load data '''\n",
    "    if dataset == 'MNIST':\n",
    "        shape_img = (28, 28)\n",
    "        num_classes = 10\n",
    "        channel = 1\n",
    "        hidden = 588\n",
    "        dst = datasets.MNIST(root=\"MNIST/.\", download=False)\n",
    "\n",
    "    elif dataset == 'CIFAR100':\n",
    "        shape_img = (32, 32)\n",
    "        num_classes = 100\n",
    "        channel = 3\n",
    "        hidden = 768\n",
    "        dst = datasets.CIFAR100(root=\"CIFAR100/.\", download=False)\n",
    "\n",
    "    elif dataset == 'LFW':\n",
    "        shape_img = (32, 32)\n",
    "        num_classes = 5749\n",
    "        channel = 3\n",
    "        hidden = 768\n",
    "        lfw_path = os.path.join(root_path, 'LFW/lfw-py/lfw_funneled')\n",
    "        dst = lfw_dataset(lfw_path, shape_img)\n",
    "\n",
    "    else:\n",
    "        exit('unknown dataset')\n",
    "\n",
    "    # varibale for final draw model\n",
    "    label_count_DLG = 0\n",
    "    label_count_iDLG = 0\n",
    "    \n",
    "    skip_exp = 0\n",
    "    size_MSE = (num_exp - skip_exp) * Iteration\n",
    "    array_MSE = [\"0.01\", \"0.005\", \"0.001\", \"0.0005\", \"0.0001\"]\n",
    "    array_MSE_DLG = [0, 0, 0, 0, 0]\n",
    "    array_MSE_iDLG = [0, 0, 0, 0, 0]\n",
    "\n",
    "    ''' train DLG and iDLG '''\n",
    "    for idx_net in range(num_exp):\n",
    "        cur_savepath = save_path + f\"/exp_{idx_net}\"\n",
    "        \n",
    "        os.makedirs(cur_savepath, exist_ok=True)\n",
    "\n",
    "        net = LeNet(channel=channel, hideen=hidden, num_classes=num_classes)\n",
    "        net.apply(weights_init)\n",
    "\n",
    "        print('running %d|%d experiment'%(idx_net, num_exp))\n",
    "        net = net.to(device)\n",
    "        idx_shuffle = np.random.permutation(len(dst))\n",
    "\n",
    "        loss_values_DLG = []\n",
    "        loss_values_iDLG = []\n",
    "        mse_values_DLG = []\n",
    "        mse_values_iDLG = []\n",
    "\n",
    "        step = 30\n",
    "        check_skip = False\n",
    "\n",
    "        for method in ['DLG', 'iDLG']:\n",
    "            print('%s, Try to generate %d images' % (method, num_dummy))\n",
    "\n",
    "            criterion = nn.CrossEntropyLoss().to(device)\n",
    "            imidx_list = []\n",
    "\n",
    "            for imidx in range(num_dummy):\n",
    "                idx = idx_shuffle[imidx]\n",
    "                imidx_list.append(idx)\n",
    "                tmp_datum = tt(dst[idx][0]).float().to(device)\n",
    "                tmp_datum = tmp_datum.view(1, *tmp_datum.size())\n",
    "                tmp_label = torch.Tensor([dst[idx][1]]).long().to(device)\n",
    "                tmp_label = tmp_label.view(1, )\n",
    "                if imidx == 0:\n",
    "                    gt_data = tmp_datum\n",
    "                    gt_label = tmp_label\n",
    "                else:\n",
    "                    gt_data = torch.cat((gt_data, tmp_datum), dim=0)\n",
    "                    gt_label = torch.cat((gt_label, tmp_label), dim=0)\n",
    "\n",
    "            # compute original gradient\n",
    "            out = net(gt_data)\n",
    "            y = criterion(out, gt_label)\n",
    "            dy_dx = torch.autograd.grad(y, net.parameters())\n",
    "            original_dy_dx = list((_.detach().clone() for _ in dy_dx))\n",
    "\n",
    "            # generate dummy data and label\n",
    "            dummy_data = torch.randn(gt_data.size()).to(device).requires_grad_(True)\n",
    "            dummy_label = torch.randn((gt_data.shape[0], num_classes)).to(device).requires_grad_(True)\n",
    "\n",
    "            if method == 'DLG':\n",
    "                optimizer = torch.optim.LBFGS([dummy_data, dummy_label], lr=lr)\n",
    "            elif method == 'iDLG':\n",
    "                optimizer = torch.optim.LBFGS([dummy_data, ], lr=lr)\n",
    "                # predict the ground-truth label\n",
    "                label_pred = torch.argmin(torch.sum(original_dy_dx[-2], dim=-1), dim=-1).detach().reshape((1,)).requires_grad_(False)\n",
    "\n",
    "            history = []\n",
    "            history_iters = []\n",
    "            losses = []\n",
    "            mses = []\n",
    "            train_iters = []\n",
    "\n",
    "            print('lr =', lr)\n",
    "\n",
    "            for iters in range(Iteration):\n",
    "\n",
    "                def closure():\n",
    "                    optimizer.zero_grad()\n",
    "                    pred = net(dummy_data)\n",
    "                    if method == 'DLG':\n",
    "                        dummy_loss = - torch.mean(torch.sum(torch.softmax(dummy_label, -1) * torch.log(torch.softmax(pred, -1)), dim=-1))\n",
    "                        # dummy_loss = criterion(pred, gt_label)\n",
    "                    elif method == 'iDLG':\n",
    "                        dummy_loss = criterion(pred, label_pred)\n",
    "\n",
    "                    dummy_dy_dx = torch.autograd.grad(dummy_loss, net.parameters(), create_graph=True)\n",
    "\n",
    "                    grad_diff = 0\n",
    "                    for gx, gy in zip(dummy_dy_dx, original_dy_dx):\n",
    "                        grad_diff += ((gx - gy) ** 2).sum()\n",
    "                    grad_diff.backward()\n",
    "                    return grad_diff\n",
    "\n",
    "                optimizer.step(closure)\n",
    "                current_loss = closure().item()\n",
    "                train_iters.append(iters)\n",
    "                losses.append(current_loss)\n",
    "                mses.append(torch.mean((dummy_data-gt_data)**2).item())\n",
    "\n",
    "                if iters % int(Iteration / 30) == 0:\n",
    "                    current_time = str(time.strftime(\"[%Y-%m-%d %H:%M:%S]\", time.localtime()))\n",
    "                    print(current_time, iters, 'loss = %.8f, mse = %.8f' %(current_loss, mses[-1]))\n",
    "                    history.append([tp(dummy_data[imidx].cpu()) for imidx in range(num_dummy)])\n",
    "                    history_iters.append(iters)\n",
    "\n",
    "                    for imidx in range(num_dummy):\n",
    "                        plt.figure(figsize=(12, 8))\n",
    "                        plt.subplot(3, 10, 1)\n",
    "                        plt.imshow(tp(gt_data[imidx].cpu()))\n",
    "                        for i in range(min(len(history), 29)):\n",
    "                            plt.subplot(3, 10, i + 2)\n",
    "                            plt.imshow(history[i][imidx])                            \n",
    "                            plt.title('iter=%d' % (history_iters[i]))\n",
    "                            plt.axis('off')\n",
    "                        if method == 'DLG':\n",
    "                            plt.savefig('%s/%05d_DLG_on_%s.png' % (cur_savepath, imidx_list[imidx], imidx_list))\n",
    "                            plt.close()\n",
    "                        elif method == 'iDLG':\n",
    "                            plt.savefig('%s/%05d_iDLG_on_%s.png' % (cur_savepath, imidx_list[imidx], imidx_list))\n",
    "                            plt.close()\n",
    "\n",
    "                    if current_loss < 0.000001: # converge\n",
    "                        break\n",
    "\n",
    "                # check loss (nan OR high loss)\n",
    "                if math.isnan(current_loss) or (iters >= 290 and current_loss >= 900):\n",
    "                    check_skip = True\n",
    "                    break\n",
    "\n",
    "                if method == 'DLG':\n",
    "                    mse_values_DLG.append(mses[-1])\n",
    "                    loss_values_DLG.append(current_loss)\n",
    "                    \n",
    "                    for i, value in enumerate(array_MSE):\n",
    "                        if mses[-1] <= float(value):\n",
    "                            array_MSE_DLG[i] += 1\n",
    "                    \n",
    "                elif method == 'iDLG':\n",
    "                    mse_values_iDLG.append(mses[-1])\n",
    "                    loss_values_iDLG.append(current_loss)\n",
    "                    \n",
    "                    for i, value in enumerate(array_MSE):\n",
    "                        if mses[-1] <= float(value):\n",
    "                            array_MSE_iDLG[i] += 1\n",
    "\n",
    "            if check_skip:\n",
    "                break\n",
    "\n",
    "            if method == 'DLG':\n",
    "                loss_DLG = losses\n",
    "                label_DLG = torch.argmax(dummy_label, dim=-1).detach().item()\n",
    "                mse_DLG = mses\n",
    "\n",
    "                create_Visualize_Loss(\"MSE\", method, history_iters, mses, [img[0] for img in history], cur_savepath, imidx_list[imidx], imidx_list)\n",
    "                create_Visualize_Loss(\"Loss\", method, history_iters, losses, [img[0] for img in history], cur_savepath, imidx_list[imidx], imidx_list)\n",
    "\n",
    "            elif method == 'iDLG':\n",
    "                loss_iDLG = losses\n",
    "                label_iDLG = label_pred.item()\n",
    "                mse_iDLG = mses\n",
    "\n",
    "                create_Visualize_Loss(\"MSE\", method, history_iters, mses, [img[0] for img in history], cur_savepath, imidx_list[imidx], imidx_list)\n",
    "                create_Visualize_Loss(\"Loss\", method, history_iters, losses, [img[0] for img in history], cur_savepath, imidx_list[imidx], imidx_list)\n",
    "\n",
    "        if check_skip:\n",
    "            skip_exp += 1\n",
    "\n",
    "            # Thông báo exp đã được skip\n",
    "            plt.figure(figsize=(4, 4))\n",
    "            plt.text(0.5, 0.5, \"Skip\", fontsize=20, ha='center', va='center')\n",
    "            plt.axis('off')\n",
    "            plt.savefig('%s/skip.png' % (cur_savepath))\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\" ============ Skip Exp {idx_net} ============ \\n\")\n",
    "            continue\n",
    "\n",
    "        # Draw MSE\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(np.arange(5, step + 1), mse_values_DLG[::10][4:], 'go-', label='DLG')\n",
    "        plt.plot(np.arange(5, step + 1), mse_values_iDLG[::10][4:], 'r*-', label='iDLG')\n",
    "        # plt.ticklabel_format(style='plain', axis='x')\n",
    "        plt.xlabel('step of Iteration')\n",
    "        plt.ylabel('Fidelity Threshold (MSE)')\n",
    "        plt.title(dataset)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig('%s/%05d_MSE_DLG_and_iDLG.png' % (cur_savepath, imidx_list[imidx]))\n",
    "        plt.close()\n",
    "\n",
    "        # Draw Loss\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(np.arange(5, step + 1), loss_values_DLG[::10][4:], 'go-', label='DLG')\n",
    "        plt.plot(np.arange(5, step + 1), loss_values_iDLG[::10][4:], 'r*-', label='iDLG')\n",
    "        # plt.ticklabel_format(style='plain', axis='x')\n",
    "        plt.xlabel('step of Iteration')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(dataset)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig('%s/%05d_Loss_DLG_and_iDLG.png' % (cur_savepath, imidx_list[imidx]))\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"\\n== Save Success grid ==\\n\")\n",
    "        \n",
    "        print('-------- INFO --------')\n",
    "        lab_gt = gt_label.detach().cpu().data.numpy()\n",
    "        print('imidx_list:', imidx_list)\n",
    "        print('loss_DLG:', loss_DLG[-1], 'loss_iDLG:', loss_iDLG[-1])\n",
    "        print('mse_DLG:', mse_DLG[-1], 'mse_iDLG:', mse_iDLG[-1])\n",
    "        print('gt_label:', lab_gt, 'lab_DLG:', label_DLG, 'lab_iDLG:', label_iDLG)\n",
    "        print('----------------------\\n\\n')\n",
    "\n",
    "        if lab_gt == label_DLG:\n",
    "            label_count_DLG += 1\n",
    "        \n",
    "        if lab_gt == label_iDLG:\n",
    "            label_count_iDLG += 1\n",
    "\n",
    "    size_MSE = (num_exp - skip_exp) * Iteration\n",
    "\n",
    "    # vẽ biểu đồ chính xác (label)\n",
    "    cities = ['DLG', 'iDLG']\n",
    "    percentages = [label_count_DLG * 100 / (num_exp - skip_exp), label_count_iDLG * 100 / (num_exp - skip_exp)]  \n",
    "    plt.bar(cities, percentages, color=['blue', 'red'])\n",
    "    plt.title(f'Tỉ lệ dự đoán đúng nhãn dataset {dataset}')\n",
    "    plt.xlabel('Mô hình tấn công')\n",
    "    plt.ylabel('Tỉ lệ chính xác (%)')\n",
    "    for i, value in enumerate(percentages):\n",
    "        plt.text(i, value + 1, f\"{value}%\", ha='center')\n",
    "    plt.savefig('%s/Accuracy_Label_on_DLG_iDLG.png' % (save_path))\n",
    "    plt.close()\n",
    "\n",
    "    # vẽ biểu đồ tỉ lệ MSE\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(array_MSE, [x * 100 / size_MSE for x in array_MSE_DLG], 'go-', label='DLG')\n",
    "    plt.plot(array_MSE, [x * 100 / size_MSE for x in array_MSE_iDLG], 'r*-', label='iDLG')\n",
    "    plt.xlabel('Fidelity Threshold (MSE)')\n",
    "    plt.ylabel('% Good Fidelity')\n",
    "    plt.xticks(array_MSE)\n",
    "    plt.yticks([0, 25, 50, 75, 100])\n",
    "    plt.title(f\"dataset {dataset}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('%s/Good_Fidelity_DLG_and_iDLG.png' % (save_path))\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"\\n\\n ================= skip: {skip_exp} ================= \\n\\n\")\n",
    "\n",
    "    cities = [\"Skip (bad)\", \"Dont Skip (good)\"]\n",
    "    percentages = [skip_exp * 100 / num_exp, (num_exp - skip_exp) * 100 / num_exp]  \n",
    "    plt.bar(cities, percentages, color=['blue', 'red'])\n",
    "    plt.title(f'Tỉ lệ skip trên dataset {dataset}')\n",
    "    plt.xlabel('Thử nghiệm (Exp)')\n",
    "    plt.ylabel('Tỉ lệ xuất hiện trên tất cả thử nghiệm (%)')\n",
    "    for i, value in enumerate(percentages):\n",
    "        plt.text(i, value + 1, f\"{value}%\", ha='center')\n",
    "    plt.savefig('%s/per_skip_on_DLG_iDLG.png' % (save_path))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    for dataset in [\"MNIST\", \"CIFAR100\", \"LFW\"]:    \n",
    "        main(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(\"MNIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(\"CIFAR100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LFW root_path: .\n",
      "LFW data_path: ./../data\n",
      "LFW save_path: ./results/iDLG_LFW\n",
      "running 0|80 experiment\n",
      "DLG, Try to generate 1 images\n",
      "lr = 1.0\n",
      "[2024-12-14 00:53:34] 0 loss = nan, mse = nan\n",
      " ============ Skip Exp 0 ============ \n",
      "\n",
      "running 1|80 experiment\n",
      "DLG, Try to generate 1 images\n",
      "lr = 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vstorm/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:282: RuntimeWarning: invalid value encountered in cast\n",
      "  npimg = (npimg * 255).astype(np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-14 00:53:34] 0 loss = 344.54571533, mse = 1.58828115\n",
      "[2024-12-14 00:53:38] 10 loss = 1217.54907227, mse = 259352543232.00000000\n",
      "[2024-12-14 00:53:39] 20 loss = 1217.54907227, mse = 259352543232.00000000\n",
      "[2024-12-14 00:53:39] 30 loss = 1217.54907227, mse = 259352543232.00000000\n",
      "[2024-12-14 00:53:39] 40 loss = 1217.54907227, mse = 259352543232.00000000\n",
      "[2024-12-14 00:53:40] 50 loss = 1217.54907227, mse = 259352543232.00000000\n",
      "[2024-12-14 00:53:44] 60 loss = 1217.54907227, mse = 259352543232.00000000\n",
      "[2024-12-14 00:53:44] 70 loss = 1217.54907227, mse = 259352543232.00000000\n",
      "[2024-12-14 00:53:45] 80 loss = 1217.54907227, mse = 259352543232.00000000\n",
      "[2024-12-14 00:53:45] 90 loss = 1217.54907227, mse = 259352543232.00000000\n",
      "[2024-12-14 00:53:45] 100 loss = 1217.54907227, mse = 259352543232.00000000\n",
      " ============ Skip Exp 1 ============ \n",
      "\n",
      "running 2|80 experiment\n",
      "DLG, Try to generate 1 images\n",
      "lr = 1.0\n",
      "[2024-12-14 00:53:46] 0 loss = 113.67625427, mse = 16.49711609\n",
      "[2024-12-14 00:53:54] 10 loss = 4.92549276, mse = 1.18814003\n",
      "[2024-12-14 00:54:02] 20 loss = 0.97261876, mse = 0.73469228\n",
      "[2024-12-14 00:54:11] 30 loss = 0.42452645, mse = 0.51828259\n",
      "[2024-12-14 00:54:19] 40 loss = 0.21898575, mse = 0.36851010\n",
      "[2024-12-14 00:54:25] 50 loss = 0.12152865, mse = 0.25455850\n",
      "[2024-12-14 00:54:32] 60 loss = 0.07273036, mse = 0.17888151\n",
      "[2024-12-14 00:54:39] 70 loss = 0.04907330, mse = 0.13673171\n",
      "[2024-12-14 00:54:46] 80 loss = 0.03336718, mse = 0.10383795\n",
      "[2024-12-14 00:54:52] 90 loss = 0.03715461, mse = 0.08018301\n",
      "[2024-12-14 00:54:59] 100 loss = 0.01934834, mse = 0.07245149\n",
      "[2024-12-14 00:55:06] 110 loss = 0.01588796, mse = 0.06317169\n",
      "[2024-12-14 00:55:13] 120 loss = 0.01298845, mse = 0.05437027\n",
      "[2024-12-14 00:55:19] 130 loss = 0.01092049, mse = 0.04717431\n",
      "[2024-12-14 00:55:26] 140 loss = 0.00922107, mse = 0.04135884\n",
      "[2024-12-14 00:55:33] 150 loss = 0.00809801, mse = 0.03698685\n",
      "[2024-12-14 00:55:42] 160 loss = 0.00681958, mse = 0.03224793\n",
      " ============ Skip Exp 2 ============ \n",
      "\n",
      "running 3|80 experiment\n",
      "DLG, Try to generate 1 images\n",
      "lr = 1.0\n",
      "[2024-12-14 00:55:46] 0 loss = 207.96394348, mse = 19.72497940\n",
      "[2024-12-14 00:55:51] 10 loss = 16.02479362, mse = 2.01135492\n",
      "[2024-12-14 00:55:57] 20 loss = 3.84149075, mse = 1.36531472\n",
      "[2024-12-14 00:56:03] 30 loss = 1.72558582, mse = 0.92518950\n",
      "[2024-12-14 00:56:10] 40 loss = 0.93524975, mse = 0.61929214\n",
      "[2024-12-14 00:56:16] 50 loss = 0.50275034, mse = 0.39703226\n",
      "[2024-12-14 00:56:23] 60 loss = 0.23793629, mse = 0.22770560\n",
      "[2024-12-14 00:56:30] 70 loss = 0.10895012, mse = 0.13072698\n",
      "[2024-12-14 00:56:37] 80 loss = 0.05482180, mse = 0.07791542\n",
      "[2024-12-14 00:56:45] 90 loss = 0.03171138, mse = 0.05093303\n",
      "[2024-12-14 00:56:53] 100 loss = 0.01967541, mse = 0.03552226\n",
      "[2024-12-14 00:57:01] 110 loss = 0.01370361, mse = 0.02721044\n",
      "[2024-12-14 00:57:07] 120 loss = 0.01037777, mse = 0.02231462\n",
      "[2024-12-14 00:57:15] 130 loss = 0.00797831, mse = 0.01839855\n",
      "[2024-12-14 00:57:22] 140 loss = 0.00655917, mse = 0.01586519\n",
      " ============ Skip Exp 3 ============ \n",
      "\n",
      "running 4|80 experiment\n",
      "DLG, Try to generate 1 images\n",
      "lr = 1.0\n",
      "[2024-12-14 00:57:27] 0 loss = 307.87951660, mse = 1.80762517\n",
      "[2024-12-14 00:57:35] 10 loss = 9.73215961, mse = 2.83600998\n",
      "[2024-12-14 00:57:43] 20 loss = 3.07868409, mse = 2.16646934\n",
      "[2024-12-14 00:57:52] 30 loss = 1.61968613, mse = 1.93334103\n",
      "[2024-12-14 00:57:59] 40 loss = 1.11601651, mse = 1.69187093\n",
      "[2024-12-14 00:58:06] 50 loss = 0.81275320, mse = 1.45188117\n",
      "[2024-12-14 00:58:13] 60 loss = 0.61286819, mse = 1.24275887\n",
      "[2024-12-14 00:58:20] 70 loss = 0.45562625, mse = 1.03383994\n",
      "[2024-12-14 00:58:27] 80 loss = 0.34652370, mse = 0.84245002\n",
      "[2024-12-14 00:58:33] 90 loss = 0.25109103, mse = 0.65588510\n",
      "[2024-12-14 00:58:40] 100 loss = 0.17338127, mse = 0.49281597\n",
      "[2024-12-14 00:58:46] 110 loss = 0.11730935, mse = 0.36470962\n",
      "[2024-12-14 00:58:53] 120 loss = 0.07816723, mse = 0.25663969\n",
      "[2024-12-14 00:59:00] 130 loss = 0.04883105, mse = 0.17918050\n",
      "[2024-12-14 00:59:07] 140 loss = 0.03203301, mse = 0.13519633\n",
      "[2024-12-14 00:59:13] 150 loss = 0.02285601, mse = 0.10572749\n",
      "[2024-12-14 00:59:20] 160 loss = 0.01727109, mse = 0.08549872\n",
      "[2024-12-14 00:59:28] 170 loss = 0.01384901, mse = 0.07132587\n",
      "[2024-12-14 00:59:35] 180 loss = 0.01138880, mse = 0.06047804\n",
      "[2024-12-14 00:59:44] 190 loss = 0.00901423, mse = 0.04925365\n",
      "[2024-12-14 00:59:52] 200 loss = 0.00738883, mse = 0.04143216\n",
      "[2024-12-14 00:59:59] 210 loss = 0.00622264, mse = 0.03575085\n",
      "[2024-12-14 01:00:06] 220 loss = 0.00525185, mse = 0.03091384\n",
      "[2024-12-14 01:00:13] 230 loss = 0.00455272, mse = 0.02721080\n",
      "[2024-12-14 01:00:20] 240 loss = 0.00398924, mse = 0.02406377\n",
      "[2024-12-14 01:00:28] 250 loss = 0.00356424, mse = 0.02165447\n",
      "[2024-12-14 01:00:35] 260 loss = 0.00311487, mse = 0.01911118\n",
      "[2024-12-14 01:00:42] 270 loss = 0.00279539, mse = 0.01713415\n",
      "[2024-12-14 01:00:48] 280 loss = 0.00249785, mse = 0.01565961\n",
      "[2024-12-14 01:00:54] 290 loss = 0.00225463, mse = 0.01464467\n",
      "iDLG, Try to generate 1 images\n",
      "lr = 1.0\n",
      "[2024-12-14 01:01:03] 0 loss = 27.82720757, mse = 1.15733135\n",
      "[2024-12-14 01:01:09] 10 loss = 0.84245247, mse = 0.39073202\n",
      "[2024-12-14 01:01:16] 20 loss = 0.11543562, mse = 0.13419673\n",
      "[2024-12-14 01:01:22] 30 loss = 0.03731361, mse = 0.06809561\n",
      "[2024-12-14 01:01:29] 40 loss = 0.01652720, mse = 0.03844196\n",
      "[2024-12-14 01:01:35] 50 loss = 0.00981161, mse = 0.02608377\n",
      "[2024-12-14 01:01:42] 60 loss = 0.00642222, mse = 0.01931090\n",
      "[2024-12-14 01:01:49] 70 loss = 0.00471717, mse = 0.01563433\n",
      "[2024-12-14 01:01:56] 80 loss = 0.00369357, mse = 0.01330516\n",
      "[2024-12-14 01:02:03] 90 loss = 80.00360107, mse = 1.11391592\n",
      "[2024-12-14 01:02:12] 100 loss = 0.00453848, mse = 0.00970164\n",
      "[2024-12-14 01:02:20] 110 loss = 0.00272851, mse = 0.00874397\n",
      "[2024-12-14 01:02:26] 120 loss = 0.00224026, mse = 0.00803675\n",
      "[2024-12-14 01:02:32] 130 loss = 0.00176736, mse = 0.00755278\n",
      "[2024-12-14 01:02:39] 140 loss = 0.00276002, mse = 0.00729411\n",
      "[2024-12-14 01:02:46] 150 loss = 0.00144051, mse = 0.00685650\n",
      "[2024-12-14 01:02:52] 160 loss = 0.00133918, mse = 0.00654445\n",
      "[2024-12-14 01:02:59] 170 loss = 0.00125026, mse = 0.00628609\n",
      "[2024-12-14 01:03:06] 180 loss = 0.00135305, mse = 0.00594376\n",
      "[2024-12-14 01:03:13] 190 loss = 0.00112252, mse = 0.00582218\n",
      "[2024-12-14 01:03:20] 200 loss = 0.00120480, mse = 0.00574508\n",
      "[2024-12-14 01:03:27] 210 loss = 0.00114120, mse = 0.00540752\n",
      "[2024-12-14 01:03:34] 220 loss = 0.00096034, mse = 0.00529498\n",
      "[2024-12-14 01:03:41] 230 loss = 0.00093476, mse = 0.00517266\n",
      "[2024-12-14 01:03:48] 240 loss = 0.00089251, mse = 0.00495508\n",
      "[2024-12-14 01:03:56] 250 loss = 0.00085905, mse = 0.00479219\n",
      "[2024-12-14 01:04:03] 260 loss = 0.00081933, mse = 0.00456351\n",
      "[2024-12-14 01:04:11] 270 loss = 0.00078505, mse = 0.00443943\n",
      "[2024-12-14 01:04:19] 280 loss = 0.00076092, mse = 0.00420456\n",
      "[2024-12-14 01:04:28] 290 loss = 0.00071751, mse = 0.00406307\n",
      "\n",
      "== Save Success grid ==\n",
      "\n",
      "-------- INFO --------\n",
      "imidx_list: [6266]\n",
      "loss_DLG: 0.002094048773869872 loss_iDLG: 0.0007042877259664237\n",
      "mse_DLG: 0.013390332460403442 mse_iDLG: 0.004005219787359238\n",
      "gt_label: [2534] lab_DLG: 2534 lab_iDLG: 2534\n",
      "----------------------\n",
      "\n",
      "\n",
      "running 5|80 experiment\n",
      "DLG, Try to generate 1 images\n",
      "lr = 1.0\n",
      "[2024-12-14 01:04:36] 0 loss = 244.54672241, mse = 1.86578369\n",
      "[2024-12-14 01:04:42] 10 loss = 39.19054794, mse = 10.95703030\n",
      "[2024-12-14 01:04:49] 20 loss = 23.80542374, mse = 9.88300514\n",
      "[2024-12-14 01:04:56] 30 loss = 16.65026474, mse = 9.27102089\n",
      "[2024-12-14 01:05:03] 40 loss = 12.61982059, mse = 9.12909985\n",
      "[2024-12-14 01:05:10] 50 loss = 10.10965538, mse = 9.43138695\n",
      "[2024-12-14 01:05:17] 60 loss = 7.06450367, mse = 9.94828224\n",
      "[2024-12-14 01:05:24] 70 loss = 5.59636211, mse = 10.31913757\n",
      "[2024-12-14 01:05:32] 80 loss = 4.78713369, mse = 10.66408157\n",
      "[2024-12-14 01:05:39] 90 loss = 4.25143814, mse = 11.02742195\n",
      "[2024-12-14 01:05:46] 100 loss = 3.89532471, mse = 11.37390137\n",
      "[2024-12-14 01:05:53] 110 loss = 3.57469559, mse = 11.72331619\n",
      "[2024-12-14 01:06:00] 120 loss = 3.34058166, mse = 11.90839195\n",
      "[2024-12-14 01:06:07] 130 loss = 3.14773417, mse = 12.11234093\n",
      "[2024-12-14 01:06:14] 140 loss = 2.96256804, mse = 12.37269497\n",
      "[2024-12-14 01:06:21] 150 loss = 2.79582453, mse = 12.71431732\n",
      "[2024-12-14 01:06:28] 160 loss = 2.66017771, mse = 13.04311180\n",
      "[2024-12-14 01:06:35] 170 loss = 2.53419113, mse = 13.27831841\n",
      "[2024-12-14 01:06:41] 180 loss = 2.43829393, mse = 13.45113945\n",
      "[2024-12-14 01:06:48] 190 loss = 2.34604120, mse = 13.63279629\n",
      "[2024-12-14 01:06:54] 200 loss = 2.27976060, mse = 13.84012794\n",
      "[2024-12-14 01:07:01] 210 loss = 2.21872401, mse = 14.03305244\n",
      "[2024-12-14 01:07:09] 220 loss = 2.17952585, mse = 14.16980743\n",
      "[2024-12-14 01:07:16] 230 loss = 2.15229893, mse = 14.28948975\n",
      "[2024-12-14 01:07:24] 240 loss = 2.12279916, mse = 14.45392895\n",
      "[2024-12-14 01:07:31] 250 loss = 2.08739376, mse = 14.63498878\n",
      "[2024-12-14 01:07:40] 260 loss = 2.05276322, mse = 14.77866173\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLFW\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[41], line 147\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m    144\u001b[0m     grad_diff\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grad_diff\n\u001b[0;32m--> 147\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m current_loss \u001b[38;5;241m=\u001b[39m closure()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    149\u001b[0m train_iters\u001b[38;5;241m.\u001b[39mappend(iters)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/lbfgs.py:401\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_old \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    400\u001b[0m     al[i] \u001b[38;5;241m=\u001b[39m old_stps[i]\u001b[38;5;241m.\u001b[39mdot(q) \u001b[38;5;241m*\u001b[39m ro[i]\n\u001b[0;32m--> 401\u001b[0m     \u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mold_dirs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mal\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;66;03m# multiply by initial Hessian\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;66;03m# r/d is the final direction\u001b[39;00m\n\u001b[1;32m    405\u001b[0m d \u001b[38;5;241m=\u001b[39m r \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmul(q, H_diag)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(\"LFW\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
